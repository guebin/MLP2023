[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "기계학습활용 (2023)",
    "section": "",
    "text": "질문하는 방법\n\n카카오톡: 질문하러 가기 // 학기종료이후 폐쇄함\n이메일: guebin@jbnu.ac.kr\n직접방문: 자연과학대학 본관 205호\nZoom: 카카오톡이나 이메일로 미리 시간을 정할 것\nLMS쪽지: https://ieilms.jbnu.ac.kr/\n\nreferences\n강의노트\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 4, 2023\n\n\n01wk-02: 캐글자료 다운로드\n\n\n최규빈 \n\n\n\n\nSep 4, 2023\n\n\n01wk-01: 가상환경 설정 (코랩사용자는 필요없음)\n\n\n최규빈 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/test.html",
    "href": "posts/test.html",
    "title": "01wk-02: 캐글자료 다운로드",
    "section": "",
    "text": "import os\nos.environ['KAGGLE_CONFIG_DIR'] = \"/home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\"\n\n\n!kaggle competitions download -c bike-sharing-demand\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts/kaggle.json'\nDownloading bike-sharing-demand.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 366kB/s]\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 366kB/s]\n\n\n\n!unzip bike-sharing-demand.zip -d ./data\n\nArchive:  bike-sharing-demand.zip\n  inflating: ./data/sampleSubmission.csv  \n  inflating: ./data/test.csv         \n  inflating: ./data/train.csv        \n\n\n\n!rm bike-sharing-demand.zip\n\n\n!ls ./data \n\nsampleSubmission.csv  test.csv  train.csv\n\n\n\nimport pandas as pd\n\n\npd.read_csv('./data/train.csv')\n\n\n\n\n\n\n\n\ndatetime\nseason\nholiday\nworkingday\nweather\ntemp\natemp\nhumidity\nwindspeed\ncasual\nregistered\ncount\n\n\n\n\n0\n2011-01-01 00:00:00\n1\n0\n0\n1\n9.84\n14.395\n81\n0.0000\n3\n13\n16\n\n\n1\n2011-01-01 01:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0000\n8\n32\n40\n\n\n2\n2011-01-01 02:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0000\n5\n27\n32\n\n\n3\n2011-01-01 03:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0000\n3\n10\n13\n\n\n4\n2011-01-01 04:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0000\n0\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10881\n2012-12-19 19:00:00\n4\n0\n1\n1\n15.58\n19.695\n50\n26.0027\n7\n329\n336\n\n\n10882\n2012-12-19 20:00:00\n4\n0\n1\n1\n14.76\n17.425\n57\n15.0013\n10\n231\n241\n\n\n10883\n2012-12-19 21:00:00\n4\n0\n1\n1\n13.94\n15.910\n61\n15.0013\n4\n164\n168\n\n\n10884\n2012-12-19 22:00:00\n4\n0\n1\n1\n13.94\n17.425\n61\n6.0032\n12\n117\n129\n\n\n10885\n2012-12-19 23:00:00\n4\n0\n1\n1\n13.12\n16.665\n66\n8.9981\n4\n84\n88\n\n\n\n\n10886 rows × 12 columns"
  },
  {
    "objectID": "posts/1wk_01.html",
    "href": "posts/1wk_01.html",
    "title": "01wk-01: 가상환경 설정 (코랩사용자는 필요없음)",
    "section": "",
    "text": "강의영상\n\n\n가상환경 설정\nconda create -n mp python=3.10\nconda activate mp\npip install autogluon && pip install autogluon.tabular[tabpfn,vowpalwabbit,imodels,skex,skl2onnx]\npip install pycaret[full]\nconda install -c huggingface transformers\npip install kaggle \nconda install -c conda-forge notebook"
  },
  {
    "objectID": "posts/1wk_01.html#아나콘다-설치",
    "href": "posts/1wk_01.html#아나콘다-설치",
    "title": "01wk-01: 가상환경 설정 (코랩사용자는 필요없음)",
    "section": "아나콘다 설치",
    "text": "아나콘다 설치\nconda create -n mp python=3.10\nconda activate mp\npip install autogluon && pip install autogluon.tabular[tabpfn,vowpalwabbit,imodels,skex,skl2onnx]\npip install pycaret[full]\nconda install -c huggingface transformers\npip install kaggle \nconda install -c conda-forge notebook"
  },
  {
    "objectID": "posts/01wk-1.html",
    "href": "posts/01wk-1.html",
    "title": "01wk-01: 가상환경 설정 (코랩사용자는 필요없음)",
    "section": "",
    "text": "강의영상\n\n\n가상환경 설정\nconda create -n mp python=3.10\nconda activate mp\npip install autogluon && pip install autogluon.tabular[tabpfn,vowpalwabbit,imodels,skex,skl2onnx]\npip install pycaret[full]\nconda install -c huggingface transformers\npip install kaggle \nconda install -c conda-forge notebook"
  },
  {
    "objectID": "posts/01wk-2.html",
    "href": "posts/01wk-2.html",
    "title": "01wk-02: 캐글자료 다운로드",
    "section": "",
    "text": "import os\nos.environ['KAGGLE_CONFIG_DIR'] = \"/home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\"\n\n\n!kaggle competitions download -c bike-sharing-demand\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts/kaggle.json'\nDownloading bike-sharing-demand.zip to /home/cgb2/Dropbox/07_lectures/2023-09-MP2023/posts\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 366kB/s]\n100%|█████████████████████████████████████████| 189k/189k [00:00&lt;00:00, 366kB/s]\n\n\n\n!unzip bike-sharing-demand.zip -d ./data\n\nArchive:  bike-sharing-demand.zip\n  inflating: ./data/sampleSubmission.csv  \n  inflating: ./data/test.csv         \n  inflating: ./data/train.csv        \n\n\n\n!rm bike-sharing-demand.zip\n\n\n!ls ./data \n\nsampleSubmission.csv  test.csv  train.csv\n\n\n\nimport pandas as pd\n\n\npd.read_csv('./data/train.csv')\n\n\n\n\n\n\n\n\ndatetime\nseason\nholiday\nworkingday\nweather\ntemp\natemp\nhumidity\nwindspeed\ncasual\nregistered\ncount\n\n\n\n\n0\n2011-01-01 00:00:00\n1\n0\n0\n1\n9.84\n14.395\n81\n0.0000\n3\n13\n16\n\n\n1\n2011-01-01 01:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0000\n8\n32\n40\n\n\n2\n2011-01-01 02:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0000\n5\n27\n32\n\n\n3\n2011-01-01 03:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0000\n3\n10\n13\n\n\n4\n2011-01-01 04:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0000\n0\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10881\n2012-12-19 19:00:00\n4\n0\n1\n1\n15.58\n19.695\n50\n26.0027\n7\n329\n336\n\n\n10882\n2012-12-19 20:00:00\n4\n0\n1\n1\n14.76\n17.425\n57\n15.0013\n10\n231\n241\n\n\n10883\n2012-12-19 21:00:00\n4\n0\n1\n1\n13.94\n15.910\n61\n15.0013\n4\n164\n168\n\n\n10884\n2012-12-19 22:00:00\n4\n0\n1\n1\n13.94\n17.425\n61\n6.0032\n12\n117\n129\n\n\n10885\n2012-12-19 23:00:00\n4\n0\n1\n1\n13.12\n16.665\n66\n8.9981\n4\n84\n88\n\n\n\n\n10886 rows × 12 columns\n\n\n\n\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n\ntrain_data = TabularDataset('./data/train.csv').drop(['casual','registered'],axis=1)\ntest_data = TabularDataset('./data/test.csv')\n\nLoaded data from: ./data/train.csv | Columns = 12 / 12 | Rows = 10886 -&gt; 10886\nLoaded data from: ./data/test.csv | Columns = 9 / 9 | Rows = 6493 -&gt; 6493\n\n\n\ntrain_data\n\n\n\n\n\n\n\n\ndatetime\nseason\nholiday\nworkingday\nweather\ntemp\natemp\nhumidity\nwindspeed\ncount\n\n\n\n\n0\n2011-01-01 00:00:00\n1\n0\n0\n1\n9.84\n14.395\n81\n0.0000\n16\n\n\n1\n2011-01-01 01:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0000\n40\n\n\n2\n2011-01-01 02:00:00\n1\n0\n0\n1\n9.02\n13.635\n80\n0.0000\n32\n\n\n3\n2011-01-01 03:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0000\n13\n\n\n4\n2011-01-01 04:00:00\n1\n0\n0\n1\n9.84\n14.395\n75\n0.0000\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10881\n2012-12-19 19:00:00\n4\n0\n1\n1\n15.58\n19.695\n50\n26.0027\n336\n\n\n10882\n2012-12-19 20:00:00\n4\n0\n1\n1\n14.76\n17.425\n57\n15.0013\n241\n\n\n10883\n2012-12-19 21:00:00\n4\n0\n1\n1\n13.94\n15.910\n61\n15.0013\n168\n\n\n10884\n2012-12-19 22:00:00\n4\n0\n1\n1\n13.94\n17.425\n61\n6.0032\n129\n\n\n10885\n2012-12-19 23:00:00\n4\n0\n1\n1\n13.12\n16.665\n66\n8.9981\n88\n\n\n\n\n10886 rows × 10 columns\n\n\n\n\ntest_data\n\n\n\n\n\n\n\n\ndatetime\nseason\nholiday\nworkingday\nweather\ntemp\natemp\nhumidity\nwindspeed\n\n\n\n\n0\n2011-01-20 00:00:00\n1\n0\n1\n1\n10.66\n11.365\n56\n26.0027\n\n\n1\n2011-01-20 01:00:00\n1\n0\n1\n1\n10.66\n13.635\n56\n0.0000\n\n\n2\n2011-01-20 02:00:00\n1\n0\n1\n1\n10.66\n13.635\n56\n0.0000\n\n\n3\n2011-01-20 03:00:00\n1\n0\n1\n1\n10.66\n12.880\n56\n11.0014\n\n\n4\n2011-01-20 04:00:00\n1\n0\n1\n1\n10.66\n12.880\n56\n11.0014\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6488\n2012-12-31 19:00:00\n1\n0\n1\n2\n10.66\n12.880\n60\n11.0014\n\n\n6489\n2012-12-31 20:00:00\n1\n0\n1\n2\n10.66\n12.880\n60\n11.0014\n\n\n6490\n2012-12-31 21:00:00\n1\n0\n1\n1\n10.66\n12.880\n60\n11.0014\n\n\n6491\n2012-12-31 22:00:00\n1\n0\n1\n1\n10.66\n13.635\n56\n8.9981\n\n\n6492\n2012-12-31 23:00:00\n1\n0\n1\n1\n10.66\n13.635\n65\n8.9981\n\n\n\n\n6493 rows × 9 columns\n\n\n\n\npredictor = TabularPredictor(label='count',problem_type='regression',path='./output')\npredictor.fit(train_data)\n\nWarning: path already exists! This predictor may overwrite an existing predictor! path=\"./output\"\nBeginning AutoGluon training ...\nAutoGluon will save models to \"./output/\"\nAutoGluon Version:  0.8.2\nPython Version:     3.10.12\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #26~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Jul 13 16:27:29 UTC 2\nDisk Space Avail:   317.15 GB / 490.57 GB (64.6%)\nTrain Data Rows:    10886\nTrain Data Columns: 9\nLabel Column: count\nPreprocessing data ...\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n    Available Memory:                    124420.04 MB\n    Train Data (Original)  Memory Usage: 1.52 MB (0.0% of available memory)\n    Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n    Stage 1 Generators:\n        Fitting AsTypeFeatureGenerator...\n            Note: Converting 2 features to boolean dtype as they only contain 2 unique values.\n    Stage 2 Generators:\n        Fitting FillNaFeatureGenerator...\n    Stage 3 Generators:\n        Fitting IdentityFeatureGenerator...\n        Fitting DatetimeFeatureGenerator...\n    Stage 4 Generators:\n        Fitting DropUniqueFeatureGenerator...\n    Stage 5 Generators:\n        Fitting DropDuplicatesFeatureGenerator...\n    Types of features in original data (raw dtype, special dtypes):\n        ('float', [])                      : 3 | ['temp', 'atemp', 'windspeed']\n        ('int', [])                        : 5 | ['season', 'holiday', 'workingday', 'weather', 'humidity']\n        ('object', ['datetime_as_object']) : 1 | ['datetime']\n    Types of features in processed data (raw dtype, special dtypes):\n        ('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n        ('int', [])                  : 3 | ['season', 'weather', 'humidity']\n        ('int', ['bool'])            : 2 | ['holiday', 'workingday']\n        ('int', ['datetime_as_int']) : 5 | ['datetime', 'datetime.year', 'datetime.month', 'datetime.day', 'datetime.dayofweek']\n    0.0s = Fit runtime\n    9 features in original data used to generate 13 features in processed data.\n    Train Data (Processed) Memory Usage: 0.98 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.05s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n    This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n    To change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.1, Train Rows: 9797, Val Rows: 1089\nUser-specified model hyperparameters to be fit:\n{\n    'NN_TORCH': {},\n    'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n    'CAT': {},\n    'XGB': {},\n    'FASTAI': {},\n    'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n    'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 11 L1 models ...\nFitting model: KNeighborsUnif ...\n    -109.9845    = Validation score   (-root_mean_squared_error)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: KNeighborsDist ...\n    -92.5189     = Validation score   (-root_mean_squared_error)\n    0.01s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: LightGBMXT ...\n    -135.958     = Validation score   (-root_mean_squared_error)\n    0.68s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBM ...\n    -134.0804    = Validation score   (-root_mean_squared_error)\n    0.46s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: RandomForestMSE ...\n    -122.0128    = Validation score   (-root_mean_squared_error)\n    0.83s    = Training   runtime\n    0.03s    = Validation runtime\nFitting model: CatBoost ...\n    -134.2362    = Validation score   (-root_mean_squared_error)\n    2.64s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: ExtraTreesMSE ...\n    -128.4294    = Validation score   (-root_mean_squared_error)\n    0.41s    = Training   runtime\n    0.2s     = Validation runtime\nFitting model: NeuralNetFastAI ...\n    -136.518     = Validation score   (-root_mean_squared_error)\n    4.85s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: XGBoost ...\n    -135.0751    = Validation score   (-root_mean_squared_error)\n    0.42s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: NeuralNetTorch ...\n    -139.9605    = Validation score   (-root_mean_squared_error)\n    13.4s    = Training   runtime\n    0.01s    = Validation runtime\nFitting model: LightGBMLarge ...\n    -132.1736    = Validation score   (-root_mean_squared_error)\n    0.77s    = Training   runtime\n    0.0s     = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n    -92.5189     = Validation score   (-root_mean_squared_error)\n    0.22s    = Training   runtime\n    0.0s     = Validation runtime\nAutoGluon training complete, total runtime = 25.55s ... Best model: \"WeightedEnsemble_L2\"\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./output/\")\n\n\n[1000]  valid_set's rmse: 136.065\n\n\n&lt;autogluon.tabular.predictor.predictor.TabularPredictor at 0x7fb5868457b0&gt;\n\n\n\n# 테스트 데이터에 대한 예측\npredictions = predictor.predict(train_data)\n\n\npredictor.evaluate(train_data, silent=True)\n\n{'root_mean_squared_error': -29.277866163077135,\n 'mean_squared_error': -857.1934470630571,\n 'mean_absolute_error': -5.693691668211702,\n 'r2': 0.9738742566766834,\n 'pearsonr': 0.9868632986400294,\n 'median_absolute_error': -0.0}\n\n\n\npredictor.leaderboard(train_data, silent=True)\n\n\n\n\n\n\n\n\nmodel\nscore_test\nscore_val\npred_time_test\npred_time_val\nfit_time\npred_time_test_marginal\npred_time_val_marginal\nfit_time_marginal\nstack_level\ncan_infer\nfit_order\n\n\n\n\n0\nKNeighborsDist\n-29.277866\n-92.518908\n0.017567\n0.002801\n0.010187\n0.017567\n0.002801\n0.010187\n1\nTrue\n2\n\n\n1\nWeightedEnsemble_L2\n-29.277866\n-92.518908\n0.018568\n0.003067\n0.232539\n0.001001\n0.000265\n0.222352\n2\nTrue\n12\n\n\n2\nRandomForestMSE\n-56.773851\n-122.012844\n0.204575\n0.034068\n0.830992\n0.204575\n0.034068\n0.830992\n1\nTrue\n5\n\n\n3\nExtraTreesMSE\n-59.892225\n-128.429416\n0.103146\n0.197580\n0.412226\n0.103146\n0.197580\n0.412226\n1\nTrue\n7\n\n\n4\nKNeighborsUnif\n-81.605878\n-109.984461\n0.017799\n0.002776\n0.010356\n0.017799\n0.002776\n0.010356\n1\nTrue\n1\n\n\n5\nXGBoost\n-87.981046\n-135.075087\n0.042990\n0.005289\n0.419790\n0.042990\n0.005289\n0.419790\n1\nTrue\n9\n\n\n6\nLightGBMLarge\n-95.485492\n-132.173561\n0.033391\n0.003778\n0.770536\n0.033391\n0.003778\n0.770536\n1\nTrue\n11\n\n\n7\nLightGBM\n-105.679214\n-134.080427\n0.044984\n0.005324\n0.462852\n0.044984\n0.005324\n0.462852\n1\nTrue\n4\n\n\n8\nCatBoost\n-109.803887\n-134.236163\n0.004670\n0.001508\n2.642006\n0.004670\n0.001508\n2.642006\n1\nTrue\n6\n\n\n9\nLightGBMXT\n-118.022777\n-135.958034\n0.085686\n0.008678\n0.682239\n0.085686\n0.008678\n0.682239\n1\nTrue\n3\n\n\n10\nNeuralNetFastAI\n-127.771403\n-136.518011\n0.080117\n0.009651\n4.846271\n0.080117\n0.009651\n4.846271\n1\nTrue\n8\n\n\n11\nNeuralNetTorch\n-133.776883\n-139.960481\n0.018608\n0.006260\n13.398683\n0.018608\n0.006260\n13.398683\n1\nTrue\n10\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt \nimport numpy as np\n\n\nnp.sqrt(np.mean((train_data['count'] - predictions)**2))\n\n29.277866163077135\n\n\n\nplt.plot(train_data['count'][100:200])\nplt.plot(predictions[100:200])"
  }
]